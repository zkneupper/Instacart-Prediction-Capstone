{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and modify notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "from multiprocessing import cpu_count\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Note: The xgboost package uses an older version of sklearn. \n",
    "# When you run \"from xgboost.sklearn import XGBClassifier,\"\n",
    "# a DeprecationWarning is raised. You can ignore the warning.\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Modify notebook settings\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create paths to data folders and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a variable for the project root directory\n",
    "proj_root = os.path.join(os.pardir)\n",
    "\n",
    "# Save the path to the folder that will contain the final,\n",
    "# processed data: /data/processed\n",
    "processed_data_dir = os.path.join(proj_root,\n",
    "                                \"data\",\n",
    "                                \"processed\")\n",
    "\n",
    "# Save the path to final, processed Instacart data file.\n",
    "final_csv_name = 'instacart_final.csv'\n",
    "\n",
    "final_csv_path = os.path.join(processed_data_dir,\n",
    "                              final_csv_name)\n",
    "\n",
    "# Save path to the `models` folder, where we will save the\n",
    "# pickled pipeline and grid search objects\n",
    "models_folder = os.path.join(proj_root,\n",
    "                             \"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(final_csv_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`total_buy_ratio_n5` is a linear combination of `total_buy_n5`. Therefore, we drop `total_buy_ratio_n5` from our feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('total_buy_ratio_n5', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract X and y from df\n",
    "X = df.drop('y', axis=1).values\n",
    "y = df['y'].values\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13,307,953\n",
      "9,315,567\n",
      "3,992,386\n"
     ]
    }
   ],
   "source": [
    "# Check the number of samples in each set.\n",
    "print('{:,}'.format(len(y)))\n",
    "print('{:,}'.format(len(y_train)))\n",
    "print('{:,}'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe = Pipeline([('sampler', SMOTE()),\n",
    "                 ('classifier', XGBClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create parameter grid\n",
    "param_grid = [\n",
    "    {'classifier': [XGBClassifier(objective='binary:logistic',\n",
    "                                  n_estimators=100)], \n",
    "     'sampler': [None, SMOTE(random_state=42)],\n",
    "     'classifier__n_estimators': [100],\n",
    "     'classifier__learning_rate': [0.1],\n",
    "     'classifier__gamma': [0.01],\n",
    "     'classifier__max_delta_step': [0, 1],\n",
    "     'classifier__max_depth': [3, 5],\n",
    "     'classifier__subsample': [1],\n",
    "     'classifier__reg_lambda': [1.0],\n",
    "     'classifier__reg_alpha': [0.1]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the number of cores to be used\n",
    "cores_used = cpu_count() - 1\n",
    "cores_used\n",
    "cores_used = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 164.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:\n",
      "{'classifier': XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
      "       gamma=0.01, learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
      "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
      "       objective='binary:logistic', reg_alpha=0.1, reg_lambda=1.0,\n",
      "       scale_pos_weight=1, seed=0, silent=True, subsample=1), 'classifier__gamma': 0.01, 'classifier__learning_rate': 0.1, 'classifier__max_delta_step': 0, 'classifier__max_depth': 5, 'classifier__n_estimators': 100, 'classifier__reg_alpha': 0.1, 'classifier__reg_lambda': 1.0, 'classifier__subsample': 1, 'sampler': None}\n",
      "\n",
      "Best cross-validation ROC AUC score: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Set verbosity\n",
    "verbosity = 1\n",
    "\n",
    "# Execute Grid search\n",
    "grid = GridSearchCV(pipe, param_grid, cv=3, scoring='roc_auc',\n",
    "                    verbose=verbosity, n_jobs=cores_used)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params:\\n{}\\n\".format(grid.best_params_))\n",
    "print(\"Best cross-validation ROC AUC score: {:.2f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the grid search object as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/gridsearch_pickle.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_gridsearch_file_name = 'gridsearch_pickle.pkl'\n",
    "\n",
    "full_gridsearch_path = os.path.join(models_folder,\n",
    "                                    full_gridsearch_file_name)\n",
    "\n",
    "joblib.dump(grid, full_gridsearch_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/pipeline_pickle.pkl']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pipeline_file_name = 'pipeline_pickle.pkl'\n",
    "\n",
    "best_pipeline_path = os.path.join(models_folder, \n",
    "                                  best_pipeline_file_name)\n",
    "\n",
    "joblib.dump(grid.best_estimator_, best_pipeline_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for optimal decision threshold\n",
    "We want to find a decision threshold that will maximize the f1 score of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_by_threshold(threshold, y, x, clf):\n",
    "    f1_val = f1_score(y,\n",
    "                      (clf.predict_proba(x)[:,1] < \\\n",
    "                       threshold).astype(int))\n",
    "    \n",
    "    if f1_val == 0:\n",
    "        f1_val = np.nan\n",
    "        \n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold 1...\n",
      "Processing fold 2...\n",
      "Processing fold 3...\n",
      "Best threshold:\n",
      "Mean best cross-validated threshold:\t0.60\n",
      "Mean best cross-validated f1 score:\t0.12\n"
     ]
    }
   ],
   "source": [
    "# Threshold values over which we will search\n",
    "thresholds = np.arange(1,10) / 10\n",
    "thresholds\n",
    "\n",
    "kf = KFold(n_splits=3)#5)\n",
    "kf.get_n_splits(X_train)\n",
    "\n",
    "best_thresholds = np.zeros(kf.n_splits)\n",
    "best_f1s = np.zeros(kf.n_splits)\n",
    "\n",
    "for i, (kf_train_index, kf_test_index) in enumerate(kf.split(X_train)):\n",
    "    print(\"Processing fold {}...\".format(str(i+1)))\n",
    "    X_kf_train = X_train[kf_train_index,:]\n",
    "    X_kf_test = X_train[kf_test_index]\n",
    "    y_kf_train = y_train[kf_train_index]\n",
    "    y_kf_test = y_train[kf_test_index]    \n",
    "    \n",
    "    clf.fit(X_kf_train, y_kf_train)\n",
    "    \n",
    "    f1_vals = np.zeros(len(thresholds))\n",
    "    \n",
    "    for (j, t) in enumerate(thresholds):\n",
    "        \n",
    "        #f1_vals[j] = f1_by_threshold(t)\n",
    "        \n",
    "        f1_vals[j] = f1_by_threshold(t, y_kf_test, X_kf_test, clf)\n",
    "    \n",
    "    best_thresholds[i] = thresholds[np.nanargmax(f1_vals)]\n",
    "    best_f1s[i] = np.nanmax(f1_vals)\n",
    "\n",
    "best_threshold = best_thresholds.mean()\n",
    "best_f1s_mean = best_f1s.mean()\n",
    "\n",
    "print(\"Best threshold:\")\n",
    "print(\"Mean best cross-validated threshold:\\t{:.2f}\".format(best_threshold))\n",
    "print(\"Mean best cross-validated f1 score:\\t{:.2f}\".format(best_f1s_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the optimal score to try classifier on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = grid.best_estimator_\n",
    "\n",
    "test_f1_score = f1_by_threshold(best_threshold, y_test, X_test, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_roc_auc_score = roc_auc_score(y_test,\n",
    "                                   clf.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation ROC AUC score:\t0.79\n",
      "Test ROC AUC score:\t\t\t0.78\n",
      "Mean best cross-validated f1 score:\t0.12\n",
      "Test f1 score:\t\t\t\t0.12\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation ROC AUC score:\\t{:.2f}\".format(grid.best_score_))\n",
    "print(\"Test ROC AUC score:\\t\\t\\t{:.2f}\".format(test_roc_auc_score))\n",
    "print(\"Mean best cross-validated f1 score:\\t{:.2f}\".format(best_f1s_mean))\n",
    "print(\"Test f1 score:\\t\\t\\t\\t{:.2f}\".format(test_f1_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
